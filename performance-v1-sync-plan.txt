# Plan for Sync Logic Enhancements in performance-v1

This document details the strategy for improving the data synchronization system in the Orbital8 application. The goals are to increase efficiency by reducing redundant network requests and to improve reliability, particularly for providers like OneDrive that rely on separate metadata files.

---

### 1. Problem Analysis: Sync Inefficiency and Brittleness

The current `SyncManager` is functional but has two primary weaknesses:

1.  **Inefficiency with Rapid Updates:** Certain user actions, such as clicking a star rating multiple times or rapidly adding/removing tags, can trigger a separate sync operation for each individual change. This floods the IndexedDB queue and results in an excessive number of small, inefficient API calls, wasting both client-side resources and network bandwidth.

2.  **Brittle OneDrive Metadata Handling:** The web worker's `syncOneDrive` function assumes a `.json` metadata file already exists in the `approot` for every image. If an image is processed for the first time and has metadata applied, the `PUT` request to update its non-existent metadata file will likely fail with a 404 Not Found error from the Microsoft Graph API, preventing the metadata from being saved.

---

### 2. Proposed Solutions

To address these issues, the following enhancements will be implemented:

**2.1. Client-Side Update Batching and Debouncing**

*   **Change:**
    1.  **Consolidate Operations in Worker:** The `SyncManager` web worker will be modified to first process the entire sync queue and group all operations by `fileId`. It will merge all the pending `updates` for a single file into one consolidated object. This ensures that even if ten rapid changes were queued for one file, only a single, efficient API call is made with the final, correct state.
    2.  **Debounce UI-driven Updates:** A `debounce` utility will be created. The primary `App.updateUserMetadata` method will use this utility with a delay (e.g., 750ms). This prevents the UI from flooding the sync queue in the first place during rapid-fire user input (like typing in the notes field or clicking stars). Critical, single-shot actions like moving a file will bypass the debounce for immediate queueing.

*   **Impact:**
    *   **Performance:** Drastically reduces the number of writes to IndexedDB and the number of outgoing sync requests. This improves client-side performance and leads to a more efficient and reliable sync process.
    *   **UX:** The application will feel more responsive as the background sync process is less taxed. The end result for the user is the same, but achieved with much less overhead.

**2.2. Robust "Upsert" Logic for OneDrive Metadata**

*   **Change:**
    1.  The `syncOneDrive` function inside the `SyncManager` web worker will be fundamentally refactored.
    2.  Instead of directly issuing a `PUT` request, the worker will now perform an "upsert" (update or insert) operation.
    3.  **Step 1: Get Consolidated Updates:** The worker will use the merged updates from the new batching logic.
    4.  **Step 2: Attempt to Fetch Existing Metadata:** The worker will make a `GET` request to `/me/drive/special/approot/<fileId>.json:/content` to see if a metadata file already exists.
    5.  **Step 3: Conditional Update/Create:**
        *   **If the `GET` request succeeds (200 OK):** The worker will merge the fetched metadata with the new updates, giving precedence to the new updates. It will then `PUT` the fully merged object back to the server.
        *   **If the `GET` request fails with a 404 Not Found:** This indicates no metadata file exists. The worker will proceed to `PUT` the new metadata object, effectively creating the file for the first time.
        *   **If the `GET` request fails with any other error:** The operation will fail and be scheduled for a retry, preserving the existing error-handling logic.

*   **Impact:**
    *   **Reliability:** This change makes the OneDrive sync process far more resilient. Metadata for new images will now be created reliably, eliminating a critical point of failure and potential data loss.
    *   **Data Integrity:** By fetching and merging, we ensure that no metadata is accidentally overwritten if multiple clients are interacting with the same file. The latest change always wins, but it's applied to the most recent known state of the metadata file.

---

This two-pronged approach will create a more professional, robust, and efficient synchronization system, directly improving the application's core reliability and performance.

---

### 3. File-Centric Change Journal Structure (`<fileId>.json`)

Every asset managed by the sync pipeline will have a companion journal stored at `approot/<fileId>.json`. The file is append-only aside from garbage collection that trims acknowledged entries. Its schema is intentionally explicit so the worker, the UI, and any debugging tooling can reason about the device's state without additional remote calls.

```
{
  "version": 1,
  "fileId": "<cloud provider id>",
  "deviceId": "<stable client guid>",
  "changes": [
    {
      "id": "chg-20240202T153045Z-k3j4",
      "timestamp": 1706887845000,
      "op": "update|move|delete|tag",  // normalized verb that the provider adapter understands
      "payload": { ... }                 // provider-specific payload, e.g. { updates: { stars: 4 } }
    }
  ],
  "lastAppliedChangeId": "chg-20240129T091530Z-mn12",    // highest id that the cloud has confirmed for this device
  "lastUploadedChangeId": "chg-20240202T153045Z-k3j4",    // most recent entry written by this client
  "lastAckAt": 1706887846000,                              // ms since epoch of the most recent acknowledgement
  "cloudCursor": {
    "deltaToken": "<onedrive delta token / google pageToken>",
    "etag": "<last metadata etag>",
    "lastWriterDeviceId": "<device guid that produced last cloud mutation>"
  },
  "retry": {
    "nextUploadNotBefore": 1706887900000,                  // persisted throttle window used after failures
    "attempts": 3                                          // number of consecutive attempts for the tail entry
  }
}
```

*   **`changes[]`:** Ordered chronologically by `timestamp` and monotonically by `id`. The worker appends new entries to the tail only. Entries remain until `lastAppliedChangeId` advances beyond the entry's `id`, at which point they are eligible for compaction.
*   **`lastAppliedChangeId`:** Persisted acknowledgement pointer. The worker never rewinds this value; any stale journal that reports an older acknowledgement is ignored, preventing out-of-order overwrites.
*   **`lastUploadedChangeId`:** A debugging aid that lets the UI detect whether local edits have been staged but not yet confirmed.
*   **`cloudCursor`:** Provider-specific information (delta token, ETag, last writer) that allows the worker to resume delta queries or skip unnecessary metadata GETs after restarts.
*   **`retry`:** Records backoff state so exponential delays survive page reloads and web worker restarts.

---

### 4. Journal-Oriented Worker Flow in `ui-v5.html`

`ui-v5.html` replaces the ad-hoc queue in earlier builds with a journal-driven worker. The worker is solely responsible for mutating the JSON document above and for determining which entries must be retransmitted to the provider.

#### 4.1 Appending new entries

1.  The UI posts `queue-change` messages that contain the normalized operation (`op`), the provider payload, and a proposed timestamp.
2.  The worker loads (or lazily caches) the `fileId.json` document, generates a lexicographically sortable `id` (`<ISO8601>-<4 char entropy>`), and appends the new object to `changes[]`.
3.  The worker updates `lastUploadedChangeId` and resets `retry.attempts` because the tail entry is brand new.
4.  A write-back is issued only if the appended entry changes the serialized document. The worker batches multiple appends within a single microtask to minimize PUT traffic.

#### 4.2 Determining which entries to re-send

*   On startup (and before each outbound batch) the worker filters `changes[]` to the subset whose `id` is lexicographically greater than `lastAppliedChangeId`. Only those entries are candidates for upload.
*   If an entry exists with the same `id` as `lastUploadedChangeId` but with an older `timestamp` (e.g., due to a crashed write) the worker replays it as well, ensuring idempotency.
*   The worker reuses `cloudCursor` to avoid redundant metadata fetches: if the cursor's `etag` matches the cached metadata object, no new `GET` is sent prior to a `PUT`.
*   When the worker is restarted it rehydrates `retry.nextUploadNotBefore`; the worker will not attempt to upload changes until the persisted throttle window expires, preventing flurries of PUTs after repeated crashes.

#### 4.3 Advancing the acknowledgement pointer

*   After the provider confirms a write (successful `PATCH`/`PUT` or journal merge), the worker sets `lastAppliedChangeId` to the acknowledged entry's `id` and updates `lastAckAt`.
*   The worker then drops every `changes[]` entry whose `id` is less than or equal to `lastAppliedChangeId` and compacts the array before persisting the JSON document.
*   If the provider acks multiple entries at once (e.g., batched OneDrive upload), the worker raises the pointer directly to the highest acknowledged `id`. The pointer never moves backwards.
*   When acknowledgement is delayed or fails, the worker leaves `lastAppliedChangeId` untouched and increments `retry.attempts`, adjusting the persisted `retry.nextUploadNotBefore` per the exponential backoff rules below.

---

### 5. Retry, Backoff, and Restart Semantics

*   **Exponential worker backoff:** The worker calculates `retryDelay = min(DEFAULT_RETRY_DELAY * 2^(attempts-1), MAX_RETRY_DELAY)` (5s → 10s → 20s → … capped at 60s) before the next upload attempt. The resulting delay is written to `retry.nextUploadNotBefore` so it survives UI reloads or browser restarts.
*   **Queue-level throttling:** The UI's IndexedDB-backed `syncQueue` persists `retryAfter` values for every record. `getPending()` ignores entries whose cooldown window has not elapsed, so repeated worker restarts will not spam the provider with the same failing request.
*   **Provider fetch hygiene:** The worker consults `cloudCursor.deltaToken`/`etag` and `lastAckAt` before issuing metadata reads. If neither the local journal nor the provider cursor has changed since the last poll, the worker skips the network call altogether, honoring the "no unneeded network calls" mandate.
*   **Crash-safe scheduling:** `retry.attempts` and `retry.nextUploadNotBefore` reset only after a successful acknowledgement. Any failure path leaves the persisted values untouched, ensuring that even after a forced reload the worker resumes with the same throttle horizon.
