# Plan for Sync Logic Enhancements in performance-v1

This document details the strategy for improving the data synchronization system in the Orbital8 application. The goals are to increase efficiency by reducing redundant network requests and to improve reliability, particularly for providers like OneDrive that rely on separate metadata files.

---

### 1. Problem Analysis: Sync Inefficiency and Brittleness

The current `SyncManager` is functional but has two primary weaknesses:

1.  **Inefficiency with Rapid Updates:** Certain user actions, such as clicking a star rating multiple times or rapidly adding/removing tags, can trigger a separate sync operation for each individual change. This floods the IndexedDB queue and results in an excessive number of small, inefficient API calls, wasting both client-side resources and network bandwidth.

2.  **Brittle OneDrive Metadata Handling:** The web worker's `syncOneDrive` function assumes a `.json` metadata file already exists in the `approot` for every image. If an image is processed for the first time and has metadata applied, the `PUT` request to update its non-existent metadata file will likely fail with a 404 Not Found error from the Microsoft Graph API, preventing the metadata from being saved.

---

### 2. Proposed Solutions

To address these issues, the following enhancements will be implemented:

**2.1. Client-Side Update Batching and Debouncing**

*   **Change:**
    1.  **Consolidate Operations in Worker:** The `SyncManager` web worker will be modified to first process the entire sync queue and group all operations by `fileId`. It will merge all the pending `updates` for a single file into one consolidated object. This ensures that even if ten rapid changes were queued for one file, only a single, efficient API call is made with the final, correct state.
    2.  **Debounce UI-driven Updates:** A `debounce` utility will be created. The primary `App.updateUserMetadata` method will use this utility with a delay (e.g., 750ms). This prevents the UI from flooding the sync queue in the first place during rapid-fire user input (like typing in the notes field or clicking stars). Critical, single-shot actions like moving a file will bypass the debounce for immediate queueing.

*   **Impact:**
    *   **Performance:** Drastically reduces the number of writes to IndexedDB and the number of outgoing sync requests. This improves client-side performance and leads to a more efficient and reliable sync process.
    *   **UX:** The application will feel more responsive as the background sync process is less taxed. The end result for the user is the same, but achieved with much less overhead.

**2.2. Robust "Upsert" Logic for OneDrive Metadata**

*   **Change:**
    1.  The `syncOneDrive` function inside the `SyncManager` web worker will be fundamentally refactored.
    2.  Instead of directly issuing a `PUT` request, the worker will now perform an "upsert" (update or insert) operation.
    3.  **Step 1: Get Consolidated Updates:** The worker will use the merged updates from the new batching logic.
    4.  **Step 2: Attempt to Fetch Existing Metadata:** The worker will make a `GET` request to `/me/drive/special/approot/<fileId>.json:/content` to see if a metadata file already exists.
    5.  **Step 3: Conditional Update/Create:**
        *   **If the `GET` request succeeds (200 OK):** The worker will merge the fetched metadata with the new updates, giving precedence to the new updates. It will then `PUT` the fully merged object back to the server.
        *   **If the `GET` request fails with a 404 Not Found:** This indicates no metadata file exists. The worker will proceed to `PUT` the new metadata object, effectively creating the file for the first time.
        *   **If the `GET` request fails with any other error:** The operation will fail and be scheduled for a retry, preserving the existing error-handling logic.

*   **Impact:**
    *   **Reliability:** This change makes the OneDrive sync process far more resilient. Metadata for new images will now be created reliably, eliminating a critical point of failure and potential data loss.
    *   **Data Integrity:** By fetching and merging, we ensure that no metadata is accidentally overwritten if multiple clients are interacting with the same file. The latest change always wins, but it's applied to the most recent known state of the metadata file.

---

**2.3. Bootstrapping PNG Metadata on First Sync**

*   **Change:**
    1.  When a file is seen for the first time, the worker issues a sanctioned `Range: bytes=0-65535` request against the provider's `/content` endpoint immediately after the metadata-listing call. This range is small enough to be treated as part of the initial fetch budget but large enough to contain the PNG signature, IHDR chunk, and all expected text chunks that we use for user annotations.
    2.  If the provider response includes an `ETag`, it is stored alongside the queued metadata record so that subsequent retries re-use the same conditional request headers.
    3.  If a previous desktop session seeded a local cache entry, the worker first checks that cache and skips the range request entirely when the cache entry is newer than the provider `lastModifiedDateTime`.

*   **Impact:**
    *   **Cold-start reliability:** The first-sync flow now guarantees that the bytes needed for metadata extraction are available without waiting for a background thumbnailing pass or a full-image download.
    *   **Bandwidth discipline:** Leveraging a capped range request keeps us within the provider's guidelines while still counting as part of the initial metadata fetch.

### 3. Folder Bootstrap & IndexedDB State

The first open of any folder establishes a complete local snapshot without bouncing back to the provider:

1.  Read the user-authored metadata from the provider (app properties and `<fileId>.json`) alongside the directory listing. Write the records into IndexedDB so the UI can resolve stacks, notes, and ratings without additional network chatter.
2.  For each PNG in the folder, stream the bytes from the sanctioned range request (or from a cache hit) into `PNGMetadataExtractor`. Only the textual payload is persisted, and it remains local—there is never a follow-up write of the parsed text back to the cloud.
3.  Record the folder name and the provider's completion timestamp as `cloud_last_updated_date_time`. Once the local bootstrap completes, capture the matching `local_last_updated_date_time`. Both timestamps live in IndexedDB next to the folder entry so later sessions can reason about freshness.
4.  Seed an empty change queue object in IndexedDB, annotated with the same timestamps, to guarantee that every device begins with a synchronized baseline.

#### 3.1. `PNGMetadataExtractor` implementation guide

*   The extractor operates on an `ArrayBuffer` returned from the range request (or cache). It wraps the buffer in a `DataView` and iterates chunk-by-chunk, following the PNG specification: read the 8-byte signature, then loop over `{length, type, data, crc}` tuples.
*   Only textual chunks are material to sync: `tEXt`, `zTXt`, and (for forward compatibility) `iTXt`. For `tEXt` the extractor decodes the byte payload directly as UTF-8. For `zTXt` it inflates the compressed portion using the already-bundled compression helper. `iTXt` is parsed by honoring the UTF-8 flag but ignoring language/translated keywords we do not store.
*   The parser stops immediately after all supported textual chunks have been processed or when it encounters non-text chunks beyond the initial 64 KiB window, avoiding full image decoding.
*   The parsed key/value pairs are normalized into our metadata schema and persisted exclusively to IndexedDB via the existing `metadataStore.put` helper. The extractor never writes to `localStorage`, `sessionStorage`, or the network layer, ensuring a single source of truth.

#### 3.2. Metadata cache keying

*   Cached entries are keyed by `{ fileId, pngMetaVersion }`. The `fileId` identifies the provider asset, while `pngMetaVersion` advances whenever we change the parsing rules or stored schema.
*   On subsequent openings, the worker queries IndexedDB with that composite key. If a hit is returned, the metadata is reused as-is without issuing any provider requests. A cache miss (or version mismatch) triggers a new range fetch followed by extraction, after which the updated entry is written back under the new `pngMetaVersion`.

### 4. Worker-Managed Change Propagation

*   UI interactions modify only IndexedDB: ratings, notes, and stack assignments are written immediately to the local stores.
*   Each write appends an `add`, `change`, or `delete` entry to the folder's queued transaction list and then debounces a wake-up of the sync worker (packaged as an inline Blob so it can be spawned without a network fetch).
*   When the debounce timer expires, the worker drains the queue in compact batches, merging updates per `fileId` before sending them to the provider's app data endpoints and `file-id.json` artifacts. Once a batch is pushed, the worker looks for additional entries and repeats until empty.
*   The worker never blocks rendering; it yields between batches and sleeps whenever the queue is clear. When the user navigates away or closes the tab, the remaining queue is flushed synchronously so no local edits are lost.

### 5. Usage Scenarios

1.  **Case 1 – first open on any device:** Perform one initial network fetch to build the local snapshot, write the `cloud_last_updated_date_time`, set `local_last_updated_date_time`, and seed the empty change queue. No additional network calls occur until a real change is detected; the worker sleeps once the queue is clear.
2.  **Case 2 – reopen after edits on the same device:** Replay the queued add/change/delete payloads already stored in IndexedDB, update the folder's `local_last_updated_date_time`, and render immediately without touching the network. Only the drained deltas mutate the snapshot.
3.  **Case 3 – first open on another device:** With no cache present, perform a single full download, persist the timestamps, and copy down any pending-change list so future opens remain incremental. The provider is not contacted again during that initial view.
4.  **Case 4 – subsequent opens on that other device:** Compare the stored `cloud_last_updated_date_time` against the most recent locally drained change. If synchronized, render with zero network usage; otherwise, fetch only the queued change payloads logged after the stored timestamp and merge just those entries.

### 6. Operational Imperatives

*   **No unneeded network calls:** The initial full sync executes only once per device. All future refreshes are driven by the locally recorded change list; any superfluous provider request is treated as a failure.
*   **No fingerprinting:** Hashing or otherwise fingerprinting file inventories or metadata is forbidden anywhere in the pipeline.
*   **LRU-bounded storage:** Apply an eviction policy that trims the least-recently-used folder caches and queues so IndexedDB cannot grow without bound.

This expanded plan provides deterministic bootstrapping, disciplined background propagation, and guardrails that keep the sync loop efficient and privacy-preserving across devices.
