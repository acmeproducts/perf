use ui-v4a.html to create ui-v5.html

basically the following is an ongoing dialog of how to synch changes between the client indexdb and the cloud. the idea is to load once and then keep both sides synched of add, change, delete via worker(s) that are implemented as an inline blob. it is critical that there is as little network traffic as possible. so load one time then process incremental changes in order. user interface waits for folder to be current there is a folder level last updated date and time for both cloud and index DB that is compared this is the quick lightweight method to know if a folder can just be straight loaded from local or if it needs to go through a process of identifying incremental changes. 


# Plan for Sync Logic Enhancements in performance-v1

This document details the strategy for improving the data synchronization system in the Orbital8 application. The goals are to increase efficiency by reducing redundant network requests and to improve reliability, particularly for providers like OneDrive that rely on separate metadata files.

---

### 1. Problem Analysis: Sync Inefficiency and Brittleness

The current `SyncManager` is functional but has two primary weaknesses:

1.  **Inefficiency with Rapid Updates:** Certain user actions, such as clicking a star rating multiple times or rapidly adding/removing tags, can trigger a separate sync operation for each individual change. This floods the IndexedDB queue and results in an excessive number of small, inefficient API calls, wasting both client-side resources and network bandwidth.

2.  **Brittle OneDrive Metadata Handling:** The web worker's `syncOneDrive` function assumes a `.json` metadata file already exists in the `approot` for every image. If an image is processed for the first time and has metadata applied, the `PUT` request to update its non-existent metadata file will likely fail with a 404 Not Found error from the Microsoft Graph API, preventing the metadata from being saved.

---

### 2. Proposed Solutions

#### Lifecycle-triggered flush handoff

The UI thread will request a final queue flush from the `SyncManager` worker whenever the page is about to leave the foreground. The handoff works as follows:

* **Events:** The app listens for `visibilitychange`, `pagehide`, and `beforeunload`. Any of these events indicate the user may be navigating away or the tab may be suspended, so the UI posts a `flushRequest` message to the worker.
* **Acknowledgement:** The UI blocks subsequent teardown logic until it receives a `flushAccepted` acknowledgement from the worker confirming that the flush job has started. The worker only sends the acknowledgement after it has taken a snapshot of the IndexedDB queue and scheduled the cloud writes.
* **Completion signal:** When the worker completes all writes (or determines it cannot finish), it emits a `flushComplete` (or `flushFailed`) message so the UI can log telemetry and proceed with dismissal.

If the worker detects that it cannot finish before the page unloads (for example, due to network back-pressure or the page hiding too quickly), it marks the active queue entry in IndexedDB with a `pendingFlush` flag. On the next session start, the worker immediately resumes the flagged items before processing any fresh UI updates.

We retain the dedicated worker architecture and do not escalate to a `SharedWorker` or Service Worker for this iteration. As a last resort, if the worker cannot issue the network calls in time, it packages the pending payload and asks the UI to dispatch it with `navigator.sendBeacon`. The UI is responsible only for relaying the worker-supplied blob, never for crafting or sending direct cloud writes on its own.

To address these issues, the following enhancements will be implemented:

**2.1. Client-Side Update Batching and Debouncing**

*   **Change:**
    1.  **Consolidate Operations in Worker:** The `SyncManager` web worker will be modified to first process the entire sync queue and group all operations by `fileId`. It will merge all the pending `updates` for a single file into one consolidated object. This ensures that even if ten rapid changes were queued for one file, only a single, efficient API call is made with the final, correct state.
    2.  **Debounce UI-driven Updates:** A `debounce` utility will be created. The primary `App.updateUserMetadata` method will use this utility with a delay (e.g., 750ms). This prevents the UI from flooding the sync queue in the first place during rapid-fire user input (like typing in the notes field or clicking stars). Critical, single-shot actions like moving a file will bypass the debounce for immediate queueing.

*   **Impact:**
    *   **Performance:** Drastically reduces the number of writes to IndexedDB and the number of outgoing sync requests. This improves client-side performance and leads to a more efficient and reliable sync process.
    *   **UX:** The application will feel more responsive as the background sync process is less taxed. The end result for the user is the same, but achieved with much less overhead.

**2.2. Robust "Upsert" Logic for OneDrive Metadata**

*   **Change:**
    1.  The `syncOneDrive` function inside the `SyncManager` web worker will be fundamentally refactored.
    2.  Instead of directly issuing a `PUT` request, the worker will now perform an "upsert" (update or insert) operation.
    3.  **Step 1: Get Consolidated Updates:** The worker will use the merged updates from the new batching logic.
    4.  **Step 2: Attempt to Fetch Existing Metadata:** The worker will make a `GET` request to `/me/drive/special/approot/<fileId>.json:/content` to see if a metadata file already exists.
    5.  **Step 3: Conditional Update/Create:**
        *   **If the `GET` request succeeds (200 OK):** The worker will merge the fetched metadata with the new updates, giving precedence to the new updates. It will then `PUT` the fully merged object back to the server.
        *   **If the `GET` request fails with a 404 Not Found:** This indicates no metadata file exists. The worker will proceed to `PUT` the new metadata object, effectively creating the file for the first time.
        *   **If the `GET` request fails with any other error:** The operation will fail and be scheduled for a retry, preserving the existing error-handling logic.

*   **Impact:**
    *   **Reliability:** This change makes the OneDrive sync process far more resilient. Metadata for new images will now be created reliably, eliminating a critical point of failure and potential data loss.
    *   **Data Integrity:** By fetching and merging, we ensure that no metadata is accidentally overwritten if multiple clients are interacting with the same file. The latest change always wins, but it's applied to the most recent known state of the metadata file.

### 3. Cache Eviction and Access Tracking

#### 3.1 Metadata extractor ingestion updates

1.  When the worker-side extractor parses a provider's metadata payload, normalize it into `MetadataUpdate` records keyed by the Orbital8 `fileId` so they can share the same persistence path as UI-originated edits.
2.  Preserve the timestamps supplied by the extractor (or stamp a new `localUpdatedAt`) so downstream reconciliation can distinguish fresh entries from older cache hits.
3.  Stage the normalized records in the worker's batching queue, keeping the structure compatible with the existing `dbManager` helpers (`{ id, metadata, localUpdatedAt }`).
4.  Persist the normalized entries by calling the shared IndexedDB helper—`dbManager.saveMetadataBatch(entries)` when batching is available, or `dbManager.saveMetadata(id, metadata)` for single records. The helper lives on the shared `DBManager` module and writes into the `'metadata'` object store of the `'Orbital8-Goji-V1'` database, so the extractor must route its output through it instead of a nonexistent `metadataStore.put`.

#### 3.2 Eviction policy

To keep the local data footprint bounded without dropping unsent work, the background `SyncManager` worker will enforce a documented eviction policy across the three IndexedDB stores it owns:

#### 3.1 zTXt decompression helper for PNG text cache

*   **Gap:** The existing worker bundle does not expose a deflate helper that can inflate the compressed text payloads inside PNG `zTXt` chunks. We cannot rely on a placeholder utility here; without an explicit decompressor the `pngText` store will only ever cache uncompressed `tEXt` entries.
*   **Change:** Add a dedicated `inflateZtxtChunk(compressed: Uint8Array): Promise<string>` helper to the PNG parsing module that runs inside the worker (`src/workers/png-text-extractor.ts`). If the file does not yet exist, create it alongside the rest of the worker code so the extractor can import it directly.
    *   The helper should wrap the browser's `DecompressionStream('deflate')` API (falling back to `pako.inflate` if we later decide to bundle that dependency) to produce a `Uint8Array` of decompressed bytes.
    *   After decompression, decode the bytes with `TextDecoder('utf-8')` before handing the string back to the caller.
*   **Integration:** Update the upcoming `parsePngTextChunks` routine to call `inflateZtxtChunk` whenever it encounters a `zTXt` chunk so that the resulting UTF-8 string can be stored in the `pngText` IndexedDB store alongside the existing eviction metrics.

*   **Tracked metrics:**
    *   Maintain a per-folder `lastAccessed` timestamp that is updated whenever a folder is rendered in the UI, when previews/text are read for metadata lookups, or when sync reconciliation requires the folder contents. These updates come from the existing render and lookup flows so we avoid additional cloud traffic.
    *   Track rolling totals for the number of cached folders and the estimated disk usage in megabytes, recalculated after each maintenance pass.
    *   Persist per-store maximums (e.g., 1,000 folders or 500 MB for the aggregate cache) so the worker can compare live metrics to limits each time it runs.
*   **Stores subject to eviction:**
    *   `folderCache` entries (thumbnails and manifest snippets) may be removed when they belong to fully-synced folders whose `lastAccessed` falls outside the retention window.
    *   `pngText` blobs follow the same policy, keyed by folder, so text overlays remain available for recently-touched folders.
    *   `changeQueue` items are *never* evicted while unsent; only folders with zero pending queue items qualify for eviction to guarantee outbound changes are preserved.
*   **Eviction flow:**
    *   During its scheduled maintenance sweep, the worker sorts eligible folders by `lastAccessed`, removes the coldest entries until the cache falls below both the folder-count and megabyte thresholds, and records each deletion (store, folderId, reclaimed bytes) in the worker log for auditability.
    *   Because guardrails restrict eviction to folders with no queued mutations, any deletion implicitly targets fully-synced data; the log makes these actions observable to diagnostics and QA.

---


the transcript below are refinements discussed to fill in details and elaborate on design decisions and principles.


there will be references to files that are not in scope like POC.HTML and performance.html 


in scope
performance-v1-sync-plan.txt
ui-v4a.html

Update poc.html with performance plan
acmeproducts/perf



Conversation
Diff
Logs

I have performance-v1-sync-plan.txt and poc.html I would like to update poc.html to incorporate all the aspects of the performance-v1-sync-plan.txt plan please review and ask any questions before we get started
Copy
Quick recap
I walked through performance-v1-sync-plan.txt and the current poc.html. The plan calls for substantive work around the sync worker (flush handoff, batching/debouncing, OneDrive upsert logic, cache ingestion, eviction metrics, PNG zTXt inflation, etc.), while the POC still has placeholder SyncManager/syncQueue behavior and an inline MetadataExtractor.

Clarifying questions before coding
Worker packaging & loading

The plan assumes a dedicated SyncManager web worker and mentions src/workers/png-text-extractor.ts, but poc.html is presently a single-file app. Do you want this update to keep everything inline (e.g., create workers via Blob URLs) or is it acceptable to split out separate .js/.ts worker files that the page loads?

IndexedDB queue schema

To implement the batching/pending-flush logic, I need to know the canonical shape of the syncQueue entries. Should each record look like { id, fileId, updates, pendingFlush, operationType, localUpdatedAt, ... }, or is there an existing schema elsewhere that I should mirror?

UI debounce scope

The plan says “UI-driven updates should debounce App.updateUserMetadata with ~750 ms, except for critical actions like moving a file.” Could you confirm which existing flows should bypass the debounce (e.g., stack moves, deletions, bulk tag edits, favorites toggles)?

Flush lifecycle wiring

On flushAccepted/flushComplete/flushFailed, what UI behavior do you expect beyond telemetry logging? For example, should we surface a toast, block navigation until flushComplete, or just resolve a promise silently?

navigator.sendBeacon fallback

If the worker offloads a payload via navigator.sendBeacon, what endpoint should receive it and what payload shape (e.g., POST JSON, FormData blob) do you want? Or is the expectation simply to stub the call for now?

OneDrive metadata upsert details

When creating missing metadata files, should we continue writing to /me/drive/special/approot/<fileId>.json exactly, or is there a newer path/extension you’d prefer (e.g., .metadata.json)? Also, any custom headers or concurrency controls to preserve?

Metadata extractor ingestion

The plan references MetadataUpdate records with localUpdatedAt timestamps funneled through dbManager.saveMetadataBatch. Should I add a batch API to DBManager and update both the sync worker and the inline MetadataExtractor to use it, or is there another ingestion pipeline we should hook into?

Cache-eviction metrics

For the new lastAccessed/disk-usage tracking, do you already have preferred storage keys or telemetry schemas we should append to, or should the worker persist its own metrics document inside IndexedDB/localStorage?

PNG zTXt decompression helper

Since poc.html currently parses PNG text synchronously in the main thread, would you like the new inflateZtxtChunk helper wired into this inline extractor, or should we defer to a separate worker module as the plan suggests?

Let me know the answers (or any extra guidance) and I’ll be ready to proceed.

1. inline blob 2. you design the schema to be purpose built we will dump any old databases and start fresh so do it right and don't compomise what you think is right. 3. please exercise your judgement as this is now your app and you are closest to it, maybe start with an understanding of some heavy use cases that this is meant to be a quick paced app so stack changes will come quick roll through 100 images straight through sorting them for instance one at a time say one every 3 seconds and those sessions can come in bursts- another key example: search for favorites: result 100 images: de-select 10 images that you decide are not really favorites and use bulk folder move. Another is bulk update via notes or tags- search returns 235 images they all get tagged with #tag. so there are some real world use cases for you to consider. 4. for now we will rely on silent promises kept unless we run into some needs for troubleshooting. 5. I have no idea whatyou are talkingabout I don't want no stubs, stubs is the kind that gets no love, hanging out on the passenger side his best friends ride..... this needs to be a complete solution round trip no questions no exceptions. 6. we decentralized because we were paying enourmouse performance costs with a centralized side car file i.e. need to change 1 image out of 1000 very inefficient. 7. no clue there's nothing other than what you see so we have a green field to do the right thing the right way the only guideline I can give you is that this is a personal app it is not meant to scale to millions of records an hour or even in a week or a month so keep it simple, simple to explain means its simple to understand do the right thing but don't go crazy with a technical solution when it's not needed. you will be challenged on any design decision that requires a network call so make sure they are elegantly simple and clean. 8. what you see if what you get if it's not there it's becasue you havent invented it yet. 9.  that is a hard one consider this: static metadata extracted from the png file doesn't need to go back to the cloud because it is static and immutable the source always travels with the image and it cannot change on the client side so it's in indexdb LRU style purely to eliminate redundant network calls if that extraction is done on a separaret thread there may be some advantages in that the it's less visible to the user the tradeoff being there could be a race condition with the image being dispalyed and the static metadata isn;t ready - I defer to you and I am willing to try one way if it doesn't carry the day we;ll try another.


Hopefully this gets you what you need. One final set of instructions/guidance:

There are several areas of the application that do not need to be touched in any way shape or form, there are key ones that are very easy to break by just making this one change that affects the whole init or auth cycle - please follow these guidelines: DO NOT MAKE ANY CHANGES TO Storage Provider Selection Screen, Authentication, Folder Selection, Look and Feel of UI.  1) please update comments and footers with new timestamp, bump the version, include a short description in the comments what is being done so we have a record we can use to keep all the versions straight.

Context
The sync plan introduces a number of concrete behaviors—flush handoffs, queue batching/debouncing, OneDrive metadata upserts, worker-driven cache ingestion/eviction, and PNG zTXt inflation—that do not yet exist in poc.html. The plan expects them to live primarily in a dedicated SyncManager worker and supporting IndexedDB helpers.

In contrast, poc.html still ships with placeholder implementations: the IndexedDB queue helpers simply resolve immediately, SyncManager has empty start/stop/requestSync methods, and metadata extraction runs entirely in the main thread without any compression support.

Why the earlier questions mattered
Worker packaging – The plan assumes real worker modules (e.g., src/workers/png-text-extractor.ts) to host the new batching, OneDrive upsert, and PNG inflation logic. Because poc.html is a single-page bundle, we need to know whether to keep everything inline via blob workers or reorganize files.

Queue schema & batching – Implementing “group by fileId” batching and the pendingFlush handoff requires a richer IndexedDB record structure than the current no-op helpers provide, so we must agree on the canonical shape before wiring it up.

Debounce exceptions – The plan calls for a 750 ms debounce on UI metadata updates but allows certain operations (e.g., moves) to bypass it. Knowing which user flows must remain immediate keeps us from delaying critical actions.

Flush lifecycle UX – The plan mentions flushAccepted/flushComplete/flushFailed messages but not how the UI should react (toasts, blocking, silent logging). Clarifying the expected user feedback ensures we implement the right surface.

navigator.sendBeacon fallback – The document suggests the worker may hand off payloads to the UI for a sendBeacon, but it doesn’t specify the endpoint or payload format. We need those details (or permission to stub) to complete the flow.

OneDrive upsert specifics – Implementing the refactored GET/merge/PUT flow requires agreement on the metadata path, error handling, and headers when creating missing files.

Metadata ingestion API – The plan expects the extractor to funnel normalized MetadataUpdate batches through a shared dbManager.saveMetadataBatch helper, but the current DBManager only exposes single-record helpers. We need to align on the new API.

Eviction metrics persistence – The eviction strategy calls for tracking lastAccessed, disk usage, and per-store limits, yet there’s no storage schema for those metrics today. Clarifying where to persist them keeps the implementation consistent.

PNG zTXt inflation – The current inline extractor only parses tEXt chunks and ignores compressed text, so we must decide whether to add the new inflateZtxtChunk helper inline or move extraction to a worker module as the plan suggests.

Once we have these answers, I can translate the plan into concrete updates in poc.html (and any necessary supporting files) while staying faithful to the intended architecture.

Here are my answers so far please tell me what else your missing and what is your recommendation: 1. inline blob 2. you design the schema to be purpose built we will dump any old databases and start fresh so do it right and don't compomise what you think is right. 3. please exercise your judgement as this is now your app and you are closest to it, maybe start with an understanding of some heavy use cases that this is meant to be a quick paced app so stack changes will come quick roll through 100 images straight through sorting them for instance one at a time say one every 3 seconds and those sessions can come in bursts- another key example: search for favorites: result 100 images: de-select 10 images that you decide are not really favorites and use bulk folder move. Another is bulk update via notes or tags- search returns 235 images they all get tagged with #tag. so there are some real world use cases for you to consider. 4. for now we will rely on silent promises kept unless we run into some needs for troubleshooting. 5. I have no idea whatyou are talkingabout I don't want no stubs, stubs is the kind that gets no love, hanging out on the passenger side his best friends ride..... this needs to be a complete solution round trip no questions no exceptions. 6. we decentralized because we were paying enourmouse performance costs with a centralized side car file i.e. need to change 1 image out of 1000 very inefficient. 7. no clue there's nothing other than what you see so we have a green field to do the right thing the right way the only guideline I can give you is that this is a personal app it is not meant to scale to millions of records an hour or even in a week or a month so keep it simple, simple to explain means its simple to understand do the right thing but don't go crazy with a technical solution when it's not needed. you will be challenged on any design decision that requires a network call so make sure they are elegantly simple and clean. 8. what you see if what you get if it's not there it's becasue you havent invented it yet. 9.  that is a hard one consider this: static metadata extracted from the png file doesn't need to go back to the cloud because it is static and immutable the source always travels with the image and it cannot change on the client side so it's in indexdb LRU style purely to eliminate redundant network calls if that extraction is done on a separaret thread there may be some advantages in that the it's less visible to the user the tradeoff being there could be a race condition with the image being dispalyed and the static metadata isn;t ready - I defer to you and I am willing to try one way if it doesn't carry the day we;ll try another.


Hopefully this gets you what you need. One final set of instructions/guidance:

There are several areas of the application that do not need to be touched in any way shape or form, there are key ones that are very easy to break by just making this one change that affects the whole init or auth cycle - please follow these guidelines: DO NOT MAKE ANY CHANGES TO Storage Provider Selection Screen, Authentication, Folder Selection, Look and Feel of UI. 
core function user able to start looking and sorting and updating images in a folder stored in the cloud ehile using their laptop - device a, pick back up where they left off on their phone - device b and then add some more images to the folder and finish up in the evening on their tablet - device c - this is a super imporatant functionality. other than that 1) please update comments and footers with new timestamp, bump the version, include a short description in the comments what is being done so we have a record
Copy
Outstanding Clarifications

sendBeacon fallback target. The plan’s unload handoff expects the worker to fall back to navigator.sendBeacon, but Graph and Drive both require Authorization headers that the Beacon API cannot supply. I can instead keep the worker in charge of the network call and switch to fetch(..., { keepalive: true }) for the unload edge case, or wire the UI to call a first-party endpoint that proxies the request (if such an endpoint exists). Let me know which direction you prefer so we stay fully functional without stubs.


Implementation Recommendation

Worker packaging. Keep everything inside poc.html by constructing inline blob workers:

A primary SyncManager worker that handles queue intake, batching, cloud writes, eviction, and lifecycle flush.

A lightweight metadata-extraction helper worker that the sync worker can spawn when it needs PNG parsing without blocking the main thread. (If this proves too heavy we can fall back to parsing inside the sync worker itself.)

IndexedDB redesign.

New version of the Orbital8-Goji-V1 database with purpose-built stores:
syncQueue, metadata, folderCache, pngText, and a small syncMeta store for eviction metrics and flush bookmarks.

syncQueue schema (one record per file):
{ id, fileId, provider, operations: [{ type, payload, queuedAt }], pendingFlush, inFlightFlushId, lastMergedAt }.
Each new UI edit merges into the existing record for that fileId. Sequential operations (e.g., move + tag change) stay in order inside operations.

metadata schema: { id: fileId, metadata, provider, localUpdatedAt, lastSyncedAt, pendingOps }.

folderCache & pngText keep current payloads but gain lastAccessed, estimatedBytes, and pendingOps so the eviction sweep can safely drop fully-synced folders.

syncMeta tracks { id: 'limits', maxFolders, maxMegabytes }, { id: 'stats', folderCount, totalBytes, lastSweep }, and { id: 'pendingFlush', queueIds }.

UI debounce policy.

Wrap App.updateUserMetadata in a 750 ms debounce for steady-state edits (notes, tags, rating, color labels).

Immediate queueing (no debounce) for bulk folder moves, deletes, stack operations, and any action flagged as “critical” in the plan so fast sessions (100-image triage, 235-item bulk tagging, etc.) still feel responsive.

Worker queue + flush flow.

UI enqueues updates via an async helper that writes/merges into syncQueue and notifies the worker.

Worker processes batches grouped by fileId, consolidating operations before hitting the provider.

On visibilitychange, pagehide, or beforeunload, UI posts a flushRequest and waits for flushAccepted before continuing teardown; worker responds with flushComplete / flushFailed. If a flush can’t finish, queue entries are flagged pendingFlush so the next session resumes them immediately.

If the page is closing, worker retries outstanding requests with keepalive: true (or your preferred beacon strategy once clarified).

OneDrive upsert refactor.

Worker performs GET → merge → PUT, creating the metadata file when 404s occur, and updating metadata.lastSyncedAt when successful.

Similar pattern for Google Drive appProperties with exponential backoff on transient errors; all retries stay inside the worker to keep the UI simple.

PNG text ingestion.

Extractor handles both tEXt and zTXt; implement inflateZtxtChunk using DecompressionStream('deflate') with a small polyfill (pako) fallback inside the worker bundle.

Results flow through the same saveMetadataBatch helper so cache hydration and UI edits share identical persistence paths.

When PNG metadata is requested but not cached, the worker spawns the extractor, stores the result in pngText, and updates metadata in one transaction.

Eviction & metrics.

Worker periodically (and on startup) scans caches, sorts folders by lastAccessed, and evicts cold entries until both folder-count and byte thresholds fall under the configured limits, skipping anything that still has pendingOps.

Every sweep updates syncMeta.stats and logs actions for diagnostics (console + optional in-memory ring buffer for future telemetry).

Housekeeping.

Maintain the “no-touch” areas (provider selection, auth, folder selection UI, look/feel).

Update the header/footer comment with the new timestamp/version + short summary when we ship.

Keep everything inline to honor the single-file constraint while modularizing logic inside closures for readability.


Reworked the IndexedDB layer to version 4, adding queue/PNG/meta stores, eviction helpers, and merge-aware enqueue logic to keep pending changes, cache sizes, and metrics aligned with the new sync design.

Implemented an inline blob-based SyncManager worker that batches operations by file, negotiates access tokens, honors lifecycle flushes with keepalive fetches, and periodically enforces cache limits before acknowledging flush completion.

Moved PNG metadata parsing into a dedicated worker with zTXt inflation, wired extracted text into the new cache store, and routed UI edits through debounced/immediate helpers that queue sync jobs while updating local metadata state.

Updated footer/version annotations to record the new UTC timestamp and describe the sync/PNG overhaul per release-tracking guidance.



Introduced a SyncDiagnostics console helper with requeue support, wired into App/window accessors, to diagnose and remedy cross-device sync issues.

Logged sync worker run summaries into IndexedDB and refreshed the release annotation text to support the new diagnostics workflow.

Testing

✅ npm run build


File (1)
Can you please tell me what that means in playing terms the diagnostics can you please make it right a log file that I can feed back to you?
Copy
What the diagnostics do (in plain terms)
The built-in diagnostics tool simply inspects what the sync system already knows and prints an easy-to-read report. When you call it, the app asks IndexedDB for four things: everything still in the sync queue, every metadata record that hasn’t been pushed yet, any cached folders that are waiting to be synced, and the last worker run/limit statistics. It then highlights common problem patterns—like items stuck in the queue, metadata that never synced, or a worker failure—and suggests a short “fix checklist” for each so you know what to try next.

Running the diagnostic uses the SyncDiagnostics.log() helper (also exposed as App.runSyncDiagnostics()), which groups the results in the browser console, prints tables for each category, and pops a toast letting you know the log is ready. The helper returns the same structured report object, so you can capture it for later.

How to produce a shareable log file
Open the app in your browser and make sure it’s on the main screen (the diagnostics rely on the in-memory app state).

Open DevTools → Console.

Run:

const report = await App.runSyncDiagnostics();
copy(JSON.stringify(report, null, 2));
App.runSyncDiagnostics() triggers the console report and gives you the full data structure in report.

copy(...) copies a formatted JSON string to your clipboard so you can paste it into a text file or send it back here.

(Optional) If the report says metadata is “pending,” you can immediately requeue it by running await SyncDiagnostics.requeuePendingMetadata();, which will also toast how many items were requeued.

That JSON blob is the “log file” you can share; it contains the queue snapshot, unsynced metadata, folder cache state, last run summary, and the checklist of suggested fixes—all in one place.

Testing
⚠️ Not run (read-only review)

Create a log button in the footer as a way to open a window with the log as console is not available when on mobile
Copy
Summary

Added a styled footer log button on every screen and refreshed the version annotation so mobile users can launch diagnostics without relying on the console. 

Wired the new button into the diagnostics flow by suppressing redundant toasts, opening a formatted popup report, and initializing the listeners during app startup. 
